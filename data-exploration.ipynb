{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "# Save test and train datasets as csv for a token classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_raw.txt\", encoding=\"utf8\") as f:\n",
    "    raw_train = f.read()\n",
    "\n",
    "with open(\"data/train_clean.txt\", encoding=\"utf8\") as g:\n",
    "    clean_train = g.read()\n",
    "\n",
    "with open(\"data/train_labels.json\", encoding=\"utf8\") as h:\n",
    "    labels_train = json.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tampa Scale for Kinesiophobia\n",
      "(Miller , Kori and Todd 1991)\n",
      "1 = <a>strongly disagree</a>\n",
      "2 = <a>disagree</a>\n",
      "3 = <a>agree</a>\n",
      "4 = <a>strongly agree</a>\n",
      "1. <q>I'm afraid that I might injury myself if I exercise</q> 1 2 3 4\n",
      "2. <q>If I were to try to overcome it, my pain would\n",
      "increase</q>\n",
      "1 2 3 4\n",
      "3. <q>My body is telling me I have something\n",
      "dangerously wrong</q>\n",
      "1 2 3 4\n",
      "4. <q>My pain would probably be relieved if I were to\n",
      "exercise</q>\n",
      "1 2 3 4\n",
      "5. <q>People aren't taking my medical condition\n",
      "seriously enough</q>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_train[:515])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tampa Scale for Kinesiophobia\n",
      "(Miller , Kori and Todd 1991)\n",
      "1 = strongly disagree\n",
      "2 = disagree\n",
      "3 = agree\n",
      "4 = strongly agree\n",
      "1. I'm afraid that I might injury myself if I exercise 1 2 3 4\n",
      "2. If I were to try to overcome it, my pain would\n",
      "increase\n",
      "1 2 3 4\n",
      "3. My body is telling me I have something\n",
      "dangerously wrong\n",
      "1 2 3 4\n",
      "4. My pain would probably be relieved if I were to\n",
      "exercise\n",
      "1 2 3 4\n",
      "5. People aren't taking my medical condition\n",
      "seriously enough\n"
     ]
    }
   ],
   "source": [
    "print(clean_train[:451])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm afraid that I might injury myself if I exercise\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = labels_train[\"q\"][0]\n",
    "\n",
    "clean_train[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[127, 178]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "tree_q = IntervalTree(\n",
    "    Interval(start, end) for start, end in labels_train[\"q\"] if start != end\n",
    ")\n",
    "\n",
    "tree_a = IntervalTree(\n",
    "    Interval(start, end) for start, end in labels_train[\"a\"] if start != end\n",
    ")\n",
    "labels_train['q'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at label distribution in test set\n",
    "# Tokenize the text then evaluate label distribution\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "label_list = [\"other\", \"question\", \"answer\"]\n",
    "\n",
    "id2label = {k: v for k, v in enumerate(label_list)}\n",
    "label2id = {v: k for k, v in enumerate(label_list)}\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 32\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer, tree_q, tree_a):\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        add_special_tokens=True,  # Includes the [CLS] and [SEP] tokens\n",
    "    )\n",
    "\n",
    "    all_token_labels = []\n",
    "    for batch_index, (input_ids, offsets) in enumerate(\n",
    "        zip(encodings[\"input_ids\"], encodings[\"offset_mapping\"])\n",
    "    ):\n",
    "        word_ids = encodings.word_ids(batch_index=batch_index)\n",
    "\n",
    "        token_labels = []\n",
    "        current_word_idx = None\n",
    "\n",
    "        for word_id, (start, end) in zip(word_ids, offsets):\n",
    "            if word_id is None:  # Special tokens like [CLS] or [SEP]\n",
    "                token_labels.append(-100)\n",
    "            elif word_id != current_word_idx:  # New word\n",
    "                if len(tree_q.overlap(start, end)) > 0:\n",
    "                    label = \"question\"\n",
    "                elif len(tree_a.overlap(start, end)) > 0:\n",
    "                    label = \"answer\"\n",
    "                else:\n",
    "                    label = \"other\"\n",
    "\n",
    "                token_labels.append(label2id[label])\n",
    "                current_word_idx = word_id\n",
    "            else:  # Subword token\n",
    "                token_labels.append(-100)\n",
    "\n",
    "        all_token_labels.append(token_labels)\n",
    "\n",
    "    encodings[\"labels\"] = all_token_labels\n",
    "\n",
    "    return encodings\n",
    "\n",
    "tokenized_dataset = tokenize(clean_train, tokenizer, tree_q, tree_a)\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": tokenized_dataset[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_dataset[\"attention_mask\"],\n",
    "        \"labels\": tokenized_dataset[\"labels\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "training_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tokens = []\n",
    "out_labels = []\n",
    "for i, (input_ids, labels) in enumerate(  # type: ignore\n",
    "    zip(training_dataset[\"input_ids\"], training_dataset[\"labels\"])  # type: ignore\n",
    "):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    for token, label in zip(tokens, labels):\n",
    "        real_label = id2label.get(label)\n",
    "        # print(f\"Token: {token:<20} Label: {real_label}\")\n",
    "        out_tokens.append(token)\n",
    "        out_labels.append(real_label)\n",
    "\n",
    "    # if i > 50:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'tokens': out_tokens, 'labels': out_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389115"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(df['labels'])\n",
    "out_labels == 'question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  22.370507433535074\n",
      "answer:  11.608650398982306\n",
      "other:  59.90928131786233\n",
      "None:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('question: ', 100*sum(df['labels'] == 'question') / total)\n",
    "print('answer: ', 100*sum(df['labels'] == 'answer') / total)\n",
    "print('other: ', 100*sum(df['labels'] == 'other') / total)\n",
    "print('None: ', 100*sum(df['labels'] == None) / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
